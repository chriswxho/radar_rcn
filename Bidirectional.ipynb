{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a161a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f307dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0451637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/Volumes/PassportDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb00dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = 'idrad'\n",
    "DATA_DIR = '/Users/davidzhu/Desktop/IDRad/idrad'\n",
    "DEFAULT_FILE = 'train/target5_001.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011a7c0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a036b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefcc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_doppler(data, chirps=256,\n",
    "                  samples=256,\n",
    "                  fft_rangesamples=2 ** 10,\n",
    "                  fft_dopplersamples=2 ** 8,\n",
    "                  fs=2.0e6,\n",
    "                  kf=1171875.0e7,\n",
    "                  min_range=0.5,\n",
    "                  max_range=10):\n",
    "    \"\"\"\n",
    "    Computes a range-doppler map for a given number of chirps and samples per chirp.\n",
    "    :param data: FMCW radar data frame consisting of <chirps>x<samples>\n",
    "    :param chirps: Number of chirps (Np)\n",
    "    :param samples: Number of samples (N)\n",
    "    :param fft_rangesamples: Number of samples for the range fft.\n",
    "    :param fft_dopplersamples: Number of samples for the doppler fft.\n",
    "    :param fs: Constant depending on the radar recording parameters.\n",
    "    :param kf: Constant depending on the radar recording parameters.\n",
    "    :param min_range: Minimum value to take into account for the range axis in the range-doppler map.\n",
    "    :param max_range: Maximum value to take into account for the range axis in the range-doppler map.\n",
    "    :return: Returns a 2D dimensional range-doppler map representing the reflected power over all range-doppler bins.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.reshape(chirps, samples).T\n",
    "    # Ignore chirp sequence number\n",
    "    data = data[1:]\n",
    "    Ny, Nx = data.shape  # rows (N), columns (Np)\n",
    "\n",
    "    window = np.hanning(Ny)\n",
    "    scaled = np.sum(window)\n",
    "    window2d = np.tile(window, (Nx, 1)).T\n",
    "    data = data * window2d\n",
    "\n",
    "    # Calculate Range FFT\n",
    "    x = np.zeros((fft_rangesamples, Nx))\n",
    "    start_index = int((fft_rangesamples - Ny) / 2)\n",
    "    x[start_index:start_index + Ny, :] = data\n",
    "    X = np.fft.fft(x, fft_rangesamples, 0) / scaled * (2.0 / 2048)\n",
    "    # Extract positive range bins\n",
    "    X = X[0:fft_rangesamples // 2, :]\n",
    "    # Extract range\n",
    "    _freq = np.arange(fft_rangesamples // 2) / float(fft_rangesamples) * fs\n",
    "    _range = _freq * 3e8 / (2 * kf)\n",
    "    min_index = np.argmin(np.abs(_range - min_range))\n",
    "    max_index = np.argmin(np.abs(_range - max_range))\n",
    "\n",
    "    X = X[min_index: max_index, :]\n",
    "\n",
    "    # Calculate Doppler FFT\n",
    "    Ny, Nx = X.shape\n",
    "    window = np.hanning(Nx)\n",
    "    scaled = np.sum(window)\n",
    "    window2d = np.tile(window, (Ny, 1))\n",
    "    X = X * window2d\n",
    "\n",
    "    rd = np.zeros((Ny, fft_dopplersamples), dtype='complex_')\n",
    "    start_index = int((fft_dopplersamples - Nx) / 2)\n",
    "    rd[:, start_index:start_index + Nx] = X\n",
    "\n",
    "    range_doppler = np.fft.fft(rd, fft_dopplersamples, 1) / scaled\n",
    "    range_doppler = np.fft.fftshift(range_doppler, axes=1)\n",
    "\n",
    "    return np.abs(range_doppler)\n",
    "\n",
    "def preprocess_file(fname): \n",
    "    with h5py.File(f'{DATA_DIR}/{fname}', 'r+') as file:\n",
    "        nframes = file['radar'].shape[0]\n",
    "\n",
    "        # Create datasets\n",
    "        if not 'microdoppler' in file:\n",
    "            file.create_dataset(\"microdoppler\", (nframes, 256), dtype='float32', chunks=(1, 256))\n",
    "        if not 'microdoppler_thresholded' in file:\n",
    "            file.create_dataset(\"microdoppler_thresholded\", (nframes, 256), dtype='float32', chunks=(1, 256))\n",
    "        if not 'range_doppler' in file:\n",
    "            file.create_dataset(\"range_doppler\", (nframes, 380, 256), dtype='float32', chunks=True)\n",
    "\n",
    "        \n",
    "        x = file['range_doppler'][:10,:,:]\n",
    "        \n",
    "        #has not been preprocessed\n",
    "        if np.all(x==0): \n",
    "            print('preprocessing')\n",
    "        \n",
    "            # Run over each radar frame\n",
    "            for i in range(nframes): # only take first 1000 \n",
    "                rd = range_doppler(file['radar'][i]) \n",
    "                rd = 20 * np.log10(rd)\n",
    "\n",
    "                file['range_doppler'][i] = rd\n",
    "                file['microdoppler'][i] = rd.sum(axis=0)\n",
    "\n",
    "                rd -= np.amax(rd)\n",
    "                rd[rd < -45] = -45\n",
    "                file['microdoppler_thresholded'][i] = rd.sum(axis=0)\n",
    "\n",
    "                if not i%100: \n",
    "                    print(\"Finished frame %d of %d.\" % (i + 1, nframes))\n",
    "                    \n",
    "def get_range_doppler(fname): \n",
    "    '''returns the range doppler'''\n",
    "    preprocess_file(fname) \n",
    "    range_doppler = 0 \n",
    "    \n",
    "    with h5py.File(f'{DATA_DIR}/{fname}', 'r+') as file:\n",
    "        # d['microdoppler'] = file['microdoppler'][:,:]\n",
    "        # d['microdoppler_thresholded'] = file['microdoppler_thresholded'][:,:]\n",
    "        range_doppler = file['range_doppler'][:,:,:].sum(axis=1)\n",
    "    \n",
    "    return range_doppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b09f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ecd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_dict = {} \n",
    "for i in range(1,6): \n",
    "    x = np.zeros(5) \n",
    "    x[i-1] = 1 \n",
    "    hot_dict[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511647f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_files, s): \n",
    "    dataset = [] \n",
    "    labels = []\n",
    "    labels_hot = [] \n",
    "    labels_repeat = [] \n",
    "    labels_hot_repeat = [] \n",
    "\n",
    "    for fname in dataset_files: \n",
    "        if fname[:2] == '._':\n",
    "            print(fname)\n",
    "            continue \n",
    "\n",
    "        labels.append(int(fname[6]))\n",
    "        labels_hot.append(copy(hot_dict[labels[-1]]))\n",
    "\n",
    "        dataset.append(get_range_doppler(f'{s}/{fname}'))\n",
    "\n",
    "        labels_repeat.append(np.repeat(labels[-1], dataset[-1].shape[0] ))\n",
    "        labels_hot_repeat.append(np.repeat(labels_hot[-1][np.newaxis, :], dataset[-1].shape[0], axis=0 ))\n",
    "\n",
    "    # dataset = np.array(dataset) \n",
    "    labels = np.array(labels)\n",
    "    labels_hot = np.array(labels_hot)\n",
    "\n",
    "    dataset = np.array(dataset, dtype = object)\n",
    "\n",
    "    labels_repeat = np.array(labels_repeat, dtype = object)\n",
    "    labels_hot_repeat = np.array(labels_hot_repeat, dtype = object)\n",
    "    \n",
    "    labels2, labels_hot2, dataset2, labels_repeat2, labels_hot_repeat2 = np.empty((labels.shape[0]), dtype = object), np.empty((labels.shape[0]), dtype = object), np.empty((labels.shape[0]), dtype = object), np.empty((labels.shape[0]), dtype = object), np.empty((labels.shape[0]), dtype = object)\n",
    "    \n",
    "    labels2[:] = list(labels)\n",
    "    labels_hot2[:] = list(labels_hot)\n",
    "    dataset2[:] = list(dataset)\n",
    "    labels_repeat2[:] = list(labels_repeat)\n",
    "    labels_hot_repeat2[:] = list(labels_hot_repeat)\n",
    "    \n",
    "    return labels2, labels_hot2, dataset2, labels_repeat2, labels_hot_repeat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5a3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_labels_hot, test_dataset, test_labels_repeat, test_labels_hot_repeat = load_data(os.listdir(f'{DATA_DIR}/test'), 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_labels_hot, train_dataset, train_labels_repeat, train_labels_hot_repeat = load_data(os.listdir(f'{DATA_DIR}/train'), 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "499d5ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing\n",
      "Finished frame 1 of 4507.\n",
      "Finished frame 101 of 4507.\n",
      "Finished frame 201 of 4507.\n",
      "Finished frame 301 of 4507.\n",
      "Finished frame 401 of 4507.\n",
      "Finished frame 501 of 4507.\n",
      "Finished frame 601 of 4507.\n",
      "Finished frame 701 of 4507.\n",
      "Finished frame 801 of 4507.\n",
      "Finished frame 901 of 4507.\n",
      "Finished frame 1001 of 4507.\n",
      "Finished frame 1101 of 4507.\n",
      "Finished frame 1201 of 4507.\n",
      "Finished frame 1301 of 4507.\n",
      "Finished frame 1401 of 4507.\n",
      "Finished frame 1501 of 4507.\n",
      "Finished frame 1601 of 4507.\n",
      "Finished frame 1701 of 4507.\n",
      "Finished frame 1801 of 4507.\n",
      "Finished frame 1901 of 4507.\n",
      "Finished frame 2001 of 4507.\n",
      "Finished frame 2101 of 4507.\n",
      "Finished frame 2201 of 4507.\n",
      "Finished frame 2301 of 4507.\n",
      "Finished frame 2401 of 4507.\n",
      "Finished frame 2501 of 4507.\n",
      "Finished frame 2601 of 4507.\n",
      "Finished frame 2701 of 4507.\n",
      "Finished frame 2801 of 4507.\n",
      "Finished frame 2901 of 4507.\n",
      "Finished frame 3001 of 4507.\n",
      "Finished frame 3101 of 4507.\n",
      "Finished frame 3201 of 4507.\n",
      "Finished frame 3301 of 4507.\n",
      "Finished frame 3401 of 4507.\n",
      "Finished frame 3501 of 4507.\n",
      "Finished frame 3601 of 4507.\n",
      "Finished frame 3701 of 4507.\n",
      "Finished frame 3801 of 4507.\n",
      "Finished frame 3901 of 4507.\n",
      "Finished frame 4001 of 4507.\n",
      "Finished frame 4101 of 4507.\n",
      "Finished frame 4201 of 4507.\n",
      "Finished frame 4301 of 4507.\n",
      "Finished frame 4401 of 4507.\n",
      "Finished frame 4501 of 4507.\n",
      "preprocessing\n",
      "Finished frame 1 of 4507.\n",
      "Finished frame 101 of 4507.\n",
      "Finished frame 201 of 4507.\n",
      "Finished frame 301 of 4507.\n",
      "Finished frame 401 of 4507.\n",
      "Finished frame 501 of 4507.\n",
      "Finished frame 601 of 4507.\n",
      "Finished frame 701 of 4507.\n",
      "Finished frame 801 of 4507.\n",
      "Finished frame 901 of 4507.\n",
      "Finished frame 1001 of 4507.\n",
      "Finished frame 1101 of 4507.\n",
      "Finished frame 1201 of 4507.\n",
      "Finished frame 1301 of 4507.\n",
      "Finished frame 1401 of 4507.\n",
      "Finished frame 1501 of 4507.\n",
      "Finished frame 1601 of 4507.\n",
      "Finished frame 1701 of 4507.\n",
      "Finished frame 1801 of 4507.\n",
      "Finished frame 1901 of 4507.\n",
      "Finished frame 2001 of 4507.\n",
      "Finished frame 2101 of 4507.\n",
      "Finished frame 2201 of 4507.\n",
      "Finished frame 2301 of 4507.\n",
      "Finished frame 2401 of 4507.\n",
      "Finished frame 2501 of 4507.\n",
      "Finished frame 2601 of 4507.\n",
      "Finished frame 2701 of 4507.\n",
      "Finished frame 2801 of 4507.\n",
      "Finished frame 2901 of 4507.\n",
      "Finished frame 3001 of 4507.\n",
      "Finished frame 3101 of 4507.\n",
      "Finished frame 3201 of 4507.\n",
      "Finished frame 3301 of 4507.\n",
      "Finished frame 3401 of 4507.\n",
      "Finished frame 3501 of 4507.\n",
      "Finished frame 3601 of 4507.\n",
      "Finished frame 3701 of 4507.\n",
      "Finished frame 3801 of 4507.\n",
      "Finished frame 3901 of 4507.\n",
      "Finished frame 4001 of 4507.\n",
      "Finished frame 4101 of 4507.\n",
      "Finished frame 4201 of 4507.\n",
      "Finished frame 4301 of 4507.\n",
      "Finished frame 4401 of 4507.\n",
      "Finished frame 4501 of 4507.\n",
      "preprocessing\n",
      "Finished frame 1 of 4507.\n",
      "Finished frame 101 of 4507.\n",
      "Finished frame 201 of 4507.\n",
      "Finished frame 301 of 4507.\n",
      "Finished frame 401 of 4507.\n",
      "Finished frame 501 of 4507.\n",
      "Finished frame 601 of 4507.\n",
      "Finished frame 701 of 4507.\n",
      "Finished frame 801 of 4507.\n",
      "Finished frame 901 of 4507.\n",
      "Finished frame 1001 of 4507.\n",
      "Finished frame 1101 of 4507.\n",
      "Finished frame 1201 of 4507.\n",
      "Finished frame 1301 of 4507.\n",
      "Finished frame 1401 of 4507.\n",
      "Finished frame 1501 of 4507.\n",
      "Finished frame 1601 of 4507.\n",
      "Finished frame 1701 of 4507.\n",
      "Finished frame 1801 of 4507.\n",
      "Finished frame 1901 of 4507.\n",
      "Finished frame 2001 of 4507.\n",
      "Finished frame 2101 of 4507.\n",
      "Finished frame 2201 of 4507.\n",
      "Finished frame 2301 of 4507.\n",
      "Finished frame 2401 of 4507.\n",
      "Finished frame 2501 of 4507.\n",
      "Finished frame 2601 of 4507.\n",
      "Finished frame 2701 of 4507.\n",
      "Finished frame 2801 of 4507.\n",
      "Finished frame 2901 of 4507.\n",
      "Finished frame 3001 of 4507.\n",
      "Finished frame 3101 of 4507.\n",
      "Finished frame 3201 of 4507.\n",
      "Finished frame 3301 of 4507.\n",
      "Finished frame 3401 of 4507.\n",
      "Finished frame 3501 of 4507.\n",
      "Finished frame 3601 of 4507.\n",
      "Finished frame 3701 of 4507.\n",
      "Finished frame 3801 of 4507.\n",
      "Finished frame 3901 of 4507.\n",
      "Finished frame 4001 of 4507.\n",
      "Finished frame 4101 of 4507.\n",
      "Finished frame 4201 of 4507.\n",
      "Finished frame 4301 of 4507.\n",
      "Finished frame 4401 of 4507.\n",
      "Finished frame 4501 of 4507.\n",
      "preprocessing\n",
      "Finished frame 1 of 4507.\n",
      "Finished frame 101 of 4507.\n",
      "Finished frame 201 of 4507.\n",
      "Finished frame 301 of 4507.\n",
      "Finished frame 401 of 4507.\n",
      "Finished frame 501 of 4507.\n",
      "Finished frame 601 of 4507.\n",
      "Finished frame 701 of 4507.\n",
      "Finished frame 801 of 4507.\n",
      "Finished frame 901 of 4507.\n",
      "Finished frame 1001 of 4507.\n",
      "Finished frame 1101 of 4507.\n",
      "Finished frame 1201 of 4507.\n",
      "Finished frame 1301 of 4507.\n",
      "Finished frame 1401 of 4507.\n",
      "Finished frame 1501 of 4507.\n",
      "Finished frame 1601 of 4507.\n",
      "Finished frame 1701 of 4507.\n",
      "Finished frame 1801 of 4507.\n",
      "Finished frame 1901 of 4507.\n",
      "Finished frame 2001 of 4507.\n",
      "Finished frame 2101 of 4507.\n",
      "Finished frame 2201 of 4507.\n",
      "Finished frame 2301 of 4507.\n",
      "Finished frame 2401 of 4507.\n",
      "Finished frame 2501 of 4507.\n",
      "Finished frame 2601 of 4507.\n",
      "Finished frame 2701 of 4507.\n",
      "Finished frame 2801 of 4507.\n",
      "Finished frame 2901 of 4507.\n",
      "Finished frame 3001 of 4507.\n",
      "Finished frame 3101 of 4507.\n",
      "Finished frame 3201 of 4507.\n",
      "Finished frame 3301 of 4507.\n",
      "Finished frame 3401 of 4507.\n",
      "Finished frame 3501 of 4507.\n",
      "Finished frame 3601 of 4507.\n",
      "Finished frame 3701 of 4507.\n",
      "Finished frame 3801 of 4507.\n",
      "Finished frame 3901 of 4507.\n",
      "Finished frame 4001 of 4507.\n",
      "Finished frame 4101 of 4507.\n",
      "Finished frame 4201 of 4507.\n",
      "Finished frame 4301 of 4507.\n",
      "Finished frame 4401 of 4507.\n",
      "Finished frame 4501 of 4507.\n",
      "preprocessing\n",
      "Finished frame 1 of 4507.\n",
      "Finished frame 101 of 4507.\n",
      "Finished frame 201 of 4507.\n",
      "Finished frame 301 of 4507.\n",
      "Finished frame 401 of 4507.\n",
      "Finished frame 501 of 4507.\n",
      "Finished frame 601 of 4507.\n",
      "Finished frame 701 of 4507.\n",
      "Finished frame 801 of 4507.\n",
      "Finished frame 901 of 4507.\n",
      "Finished frame 1001 of 4507.\n",
      "Finished frame 1101 of 4507.\n",
      "Finished frame 1201 of 4507.\n",
      "Finished frame 1301 of 4507.\n",
      "Finished frame 1401 of 4507.\n",
      "Finished frame 1501 of 4507.\n",
      "Finished frame 1601 of 4507.\n",
      "Finished frame 1701 of 4507.\n",
      "Finished frame 1801 of 4507.\n",
      "Finished frame 1901 of 4507.\n",
      "Finished frame 2001 of 4507.\n",
      "Finished frame 2101 of 4507.\n",
      "Finished frame 2201 of 4507.\n",
      "Finished frame 2301 of 4507.\n",
      "Finished frame 2401 of 4507.\n",
      "Finished frame 2501 of 4507.\n",
      "Finished frame 2601 of 4507.\n",
      "Finished frame 2701 of 4507.\n",
      "Finished frame 2801 of 4507.\n",
      "Finished frame 2901 of 4507.\n",
      "Finished frame 3001 of 4507.\n",
      "Finished frame 3101 of 4507.\n",
      "Finished frame 3201 of 4507.\n",
      "Finished frame 3301 of 4507.\n",
      "Finished frame 3401 of 4507.\n",
      "Finished frame 3501 of 4507.\n",
      "Finished frame 3601 of 4507.\n",
      "Finished frame 3701 of 4507.\n",
      "Finished frame 3801 of 4507.\n",
      "Finished frame 3901 of 4507.\n",
      "Finished frame 4001 of 4507.\n",
      "Finished frame 4101 of 4507.\n",
      "Finished frame 4201 of 4507.\n",
      "Finished frame 4301 of 4507.\n",
      "Finished frame 4401 of 4507.\n",
      "Finished frame 4501 of 4507.\n"
     ]
    }
   ],
   "source": [
    "valid_labels, valid_labels_hot, valid_dataset, valid_labels_repeat, valid_labels_hot_repeat = load_data(os.listdir(f'{DATA_DIR}/valid'), 'valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10af508",
   "metadata": {},
   "source": [
    "# Layer Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10043488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, BatchIntrinsicPlasticity, NodeToNode, HebbianNodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a79e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.echo_state_network import ESNRegressor, ESNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0467c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, PredefinedWeightsInputToNode, NodeToNode\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7258097",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rd = get_range_doppler(DEFAULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582fd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 500\n",
    "layer2_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67c80351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilayered; i2n -> n2o, second layer has fewer (125) nodes\n",
    "\n",
    "i2n = InputToNode(hidden_layer_size=layer1_size, input_activation=\"tanh\")\n",
    "\n",
    "# layer1 = i2n.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6171882",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = {'hidden_layer_size': layer2_size, #1000 in 5C\n",
    "                  'input_activation': 'tanh', #\n",
    "                  'k_in' : 1,\n",
    "                  'bias_scaling': 0.0, # usually 0 \n",
    "                  'spectral_radius' : 0.0, # \n",
    "                  'reservoir_activation': 'tanh', # 2\n",
    "                  'leakage': 0.05, #equation 6, 17 frames should be 1 step \n",
    "                  'bidirectional': True, #bidirectional \n",
    "                  'k_rec': layer2_size / 10, # optimal is 200 for 1000 layer size \n",
    "                  'alpha': 0.1, # figure 6, tuned with k_in\n",
    "                  'random_state': 1, # \n",
    "                  'requires_sequence': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51057305",
   "metadata": {},
   "outputs": [],
   "source": [
    "bid = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a03477",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Pipeline([\n",
    "    ('i2n', i2n), \n",
    "              ])\n",
    "n2n = Pipeline([('layer1', NodeToNode(hidden_layer_size=layer2_size, leakage=.05, bidirectional = bid)),\n",
    "               ('layer2', NodeToNode(hidden_layer_size=layer2_size * 2 ** bid, leakage=.05, bidirectional = bid))])\n",
    "n2o = ESNClassifier(input_to_node=l1, node_to_node=n2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69d1ed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESNClassifier(input_to_node=Pipeline(steps=[('i2n', InputToNode())]),\n",
       "              node_to_node=Pipeline(steps=[('layer1', NodeToNode(leakage=0.05)),\n",
       "                                           ('layer2',\n",
       "                                            NodeToNode(leakage=0.05))]),\n",
       "              regressor=IncrementalRegression(), requires_sequence=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2o.fit(train_dataset, train_labels_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c0e191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data, data_labels): \n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    correct = []\n",
    "    for i, prediction in enumerate(predictions):  \n",
    "\n",
    "        correct.append((prediction == data_labels[i]).mean())\n",
    "\n",
    "    correct = np.array(correct)\n",
    "    \n",
    "    print(correct.mean())\n",
    "    \n",
    "    return predictions, correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e6e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260084374556955\n"
     ]
    }
   ],
   "source": [
    "train_predictions, train_acc = get_accuracy(n2o, train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c87756ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    test_dataset[i] = test_dataset[i].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aee9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    valid_dataset[i] = valid_dataset[i].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9089216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3132904370978478\n"
     ]
    }
   ],
   "source": [
    "test_predictions, test_acc = get_accuracy(n2o, test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c02ebc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38340359440869753\n"
     ]
    }
   ],
   "source": [
    "valid_predictions, valid_acc = get_accuracy(n2o, valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3e505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913afb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cb1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35080f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3618a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f73796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6eccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48745a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a751971",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, PredefinedWeightsInputToNode, NodeToNode\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, f1_score\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5d613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': layer2_size, #1000 in 5C\n",
    "                  'input_activation': 'tanh', #\n",
    "                  'k_in' : 1,\n",
    "                  # 'bias_scaling': 0.0, # usually 0 \n",
    "                  # 'spectral_radius' : 0.0, # \n",
    "                  'reservoir_activation': 'tanh', # 2\n",
    "                  # 'leakage': 0.05, #equation 6, 17 frames should be 1 step \n",
    "                  'bidirectional': True, #bidirectional \n",
    "                  # 'k_rec': layer2_size / 10, # optimal is 200 for 1000 layer size \n",
    "                  # 'alpha': 0.1, # figure 6, tuned with k_in\n",
    "                  'random_state': 1, # \n",
    "                  'requires_sequence': True}\n",
    "\n",
    "step1_esn_params = {'k_rec': uniform(10, layer2_size/2),\n",
    "                    'spectral_radius': uniform(loc=0, scale=1e-3)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': uniform(1e-5, 1e-1)}\n",
    "\n",
    "scorer = make_scorer(accuracy_score, greater_is_better=True, needs_proba=False)\n",
    "step_kwargs = {'n_iter': 50, 'random_state': 1, 'verbose': 1, 'n_jobs': -1, 'cv': TimeSeriesSplit(), 'scoring': scorer}\n",
    "\n",
    "kwargs_step1 = {**step_kwargs}\n",
    "kwargs_step2 = {**step_kwargs}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': scorer}\n",
    "kwargs_step4 = {**step_kwargs}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNClassifier(**initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"joblib/mse_sequential_search.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(dataset, labels_repeat)\n",
    "    dump(sequential_search, \"joblib/mse_sequential_search.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c543266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "esn = sequential_search.best_estimator_\n",
    "\n",
    "y_pred_train = esn.predict_proba(X=dataset)\n",
    "cm = confusion_matrix(np.argmax(np.concatenate(labels_hot_repeat), axis=1), np.argmax(np.concatenate(y_pred_train), axis=1))\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=[0, 1, 2, 3, 4]).plot()\n",
    "print(\"Classification training report for estimator %s:\\n%s\\n\" % (esn, classification_report(np.argmax(np.concatenate(labels_hot_repeat), axis=1), np.argmax(np.concatenate(y_pred_train), axis=1), digits=10)))\n",
    "\n",
    "# y_pred_test = esn.predict(X=X_test)\n",
    "# cm = confusion_matrix(np.argmax(np.concatenate(y_test), axis=1), np.argmax(np.concatenate(y_pred_test), axis=1))\n",
    "# cm_display = ConfusionMatrixDisplay(cm, display_labels=[0, 1, 2]).plot()\n",
    "# print(\"Classification training report for estimator %s:\\n%s\\n\" % (esn, classification_report(np.argmax(np.concatenate(y_test), axis=1), np.argmax(np.concatenate(y_pred_test), axis=1), digits=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a817ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from door example\n",
    "\n",
    "\n",
    "\n",
    "initially_fixed_params = {'hidden_layer_size': 100,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage': 0.1,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bidirectional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_km_esn = ESNClassifier(input_to_node=PredefinedWeightsInputToNode(predefined_input_weights=w_in.T),\n",
    "                                    **initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"RICSyN2015/sequential_search_RICSyN2015_km_large_1.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_km_esn, searches=searches).fit(dataset, labels_repeat)\n",
    "    dump(sequential_search, \"RICSyN2015/sequential_search_RICSyN2015_km_large_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_esn.fit(dataset, labels_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = base_esn.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa18d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for i, prediction in enumerate(predictions):  \n",
    "    \n",
    "    correct.append((prediction == labels[i]).mean())\n",
    "\n",
    "correct = np.array(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeedafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
