{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a161a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f307dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0451637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/Volumes/PassportDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb00dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'idrad'\n",
    "# DATA_DIR = '/Users/davidzhu/Desktop/IDRad/idrad'\n",
    "DEFAULT_FILE = 'train/target5_001.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011a7c0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a036b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefcc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_doppler(data, chirps=256,\n",
    "                  samples=256,\n",
    "                  fft_rangesamples=2 ** 10,\n",
    "                  fft_dopplersamples=2 ** 8,\n",
    "                  fs=2.0e6,\n",
    "                  kf=1171875.0e7,\n",
    "                  min_range=0.5,\n",
    "                  max_range=10):\n",
    "    \"\"\"\n",
    "    Computes a range-doppler map for a given number of chirps and samples per chirp.\n",
    "    :param data: FMCW radar data frame consisting of <chirps>x<samples>\n",
    "    :param chirps: Number of chirps (Np)\n",
    "    :param samples: Number of samples (N)\n",
    "    :param fft_rangesamples: Number of samples for the range fft.\n",
    "    :param fft_dopplersamples: Number of samples for the doppler fft.\n",
    "    :param fs: Constant depending on the radar recording parameters.\n",
    "    :param kf: Constant depending on the radar recording parameters.\n",
    "    :param min_range: Minimum value to take into account for the range axis in the range-doppler map.\n",
    "    :param max_range: Maximum value to take into account for the range axis in the range-doppler map.\n",
    "    :return: Returns a 2D dimensional range-doppler map representing the reflected power over all range-doppler bins.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.reshape(chirps, samples).T\n",
    "    # Ignore chirp sequence number\n",
    "    data = data[1:]\n",
    "    Ny, Nx = data.shape  # rows (N), columns (Np)\n",
    "\n",
    "    window = np.hanning(Ny)\n",
    "    scaled = np.sum(window)\n",
    "    window2d = np.tile(window, (Nx, 1)).T\n",
    "    data = data * window2d\n",
    "\n",
    "    # Calculate Range FFT\n",
    "    x = np.zeros((fft_rangesamples, Nx))\n",
    "    start_index = int((fft_rangesamples - Ny) / 2)\n",
    "    x[start_index:start_index + Ny, :] = data\n",
    "    X = np.fft.fft(x, fft_rangesamples, 0) / scaled * (2.0 / 2048)\n",
    "    # Extract positive range bins\n",
    "    X = X[0:fft_rangesamples // 2, :]\n",
    "    # Extract range\n",
    "    _freq = np.arange(fft_rangesamples // 2) / float(fft_rangesamples) * fs\n",
    "    _range = _freq * 3e8 / (2 * kf)\n",
    "    min_index = np.argmin(np.abs(_range - min_range))\n",
    "    max_index = np.argmin(np.abs(_range - max_range))\n",
    "\n",
    "    X = X[min_index: max_index, :]\n",
    "\n",
    "    # Calculate Doppler FFT\n",
    "    Ny, Nx = X.shape\n",
    "    window = np.hanning(Nx)\n",
    "    scaled = np.sum(window)\n",
    "    window2d = np.tile(window, (Ny, 1))\n",
    "    X = X * window2d\n",
    "\n",
    "    rd = np.zeros((Ny, fft_dopplersamples), dtype='complex_')\n",
    "    start_index = int((fft_dopplersamples - Nx) / 2)\n",
    "    rd[:, start_index:start_index + Nx] = X\n",
    "\n",
    "    range_doppler = np.fft.fft(rd, fft_dopplersamples, 1) / scaled\n",
    "    range_doppler = np.fft.fftshift(range_doppler, axes=1)\n",
    "\n",
    "    return np.abs(range_doppler)\n",
    "\n",
    "def preprocess_file(fname): \n",
    "    with h5py.File(f'{DATA_DIR}/{fname}', 'r+') as file:\n",
    "        nframes = file['radar'].shape[0]\n",
    "\n",
    "        # Create datasets\n",
    "        if not 'microdoppler' in file:\n",
    "            file.create_dataset(\"microdoppler\", (nframes, 256), dtype='float32', chunks=(1, 256))\n",
    "        if not 'microdoppler_thresholded' in file:\n",
    "            file.create_dataset(\"microdoppler_thresholded\", (nframes, 256), dtype='float32', chunks=(1, 256))\n",
    "        if not 'range_doppler' in file:\n",
    "            file.create_dataset(\"range_doppler\", (nframes, 380, 256), dtype='float32', chunks=True)\n",
    "\n",
    "        \n",
    "        x = file['range_doppler'][:10,:,:]\n",
    "        \n",
    "        #has not been preprocessed\n",
    "        if np.all(x==0): \n",
    "            print('preprocessing')\n",
    "        \n",
    "            # Run over each radar frame\n",
    "            for i in range(nframes): # only take first 1000 \n",
    "                rd = range_doppler(file['radar'][i]) \n",
    "                rd = 20 * np.log10(rd)\n",
    "\n",
    "                file['range_doppler'][i] = rd\n",
    "                file['microdoppler'][i] = rd.sum(axis=0)\n",
    "\n",
    "                rd -= np.amax(rd)\n",
    "                rd[rd < -45] = -45\n",
    "                file['microdoppler_thresholded'][i] = rd.sum(axis=0)\n",
    "\n",
    "                if not i%100: \n",
    "                    print(\"Finished frame %d of %d.\" % (i + 1, nframes))\n",
    "                    \n",
    "def get_range_doppler(fname): \n",
    "    '''returns the range doppler'''\n",
    "    preprocess_file(fname) \n",
    "    range_doppler = 0 \n",
    "    \n",
    "    with h5py.File(f'{DATA_DIR}/{fname}', 'r+') as file:\n",
    "        # d['microdoppler'] = file['microdoppler'][:,:]\n",
    "        # d['microdoppler_thresholded'] = file['microdoppler_thresholded'][:,:]\n",
    "        range_doppler = file['range_doppler'][:,:,:].sum(axis=1)\n",
    "    \n",
    "    return range_doppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b09f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ecd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_dict = {} \n",
    "for i in range(1,6): \n",
    "    x = np.zeros(5) \n",
    "    x[i-1] = 1 \n",
    "    hot_dict[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8c7fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_files = os.listdir(f'{DATA_DIR}/train')\n",
    "dataset = [] \n",
    "labels = []\n",
    "labels_hot = [] \n",
    "labels_repeat = [] \n",
    "labels_hot_repeat = [] \n",
    "\n",
    "for fname in dataset_files: \n",
    "    if fname[:2] == '._':\n",
    "        print(fname)\n",
    "        continue \n",
    "        \n",
    "    labels.append(int(fname[6]))\n",
    "    labels_hot.append(copy(hot_dict[labels[-1]]))\n",
    "    \n",
    "    dataset.append(get_range_doppler(f'train/{fname}'))\n",
    "    \n",
    "    labels_repeat.append(np.repeat(labels[-1], dataset[-1].shape[0] ))\n",
    "    labels_hot_repeat.append(np.repeat(labels_hot[-1][np.newaxis, :], dataset[-1].shape[0], axis=0 ))\n",
    "\n",
    "# dataset = np.array(dataset) \n",
    "labels = np.array(labels)\n",
    "labels_hot = np.array(labels_hot)\n",
    "\n",
    "dataset = np.array(dataset, dtype = object)\n",
    "\n",
    "labels_repeat = np.array(labels_repeat, dtype = object)\n",
    "labels_hot_repeat = np.array(labels_hot_repeat, dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44ee442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = np.array(dataset, dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0a3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b44737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130,)\n",
      "(130,)\n",
      "(130,)\n",
      "(179, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(labels.shape)\n",
    "print(labels_repeat.shape)\n",
    "print(labels_hot_repeat[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504f2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10af508",
   "metadata": {},
   "source": [
    "# Layer Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10043488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, BatchIntrinsicPlasticity, NodeToNode, HebbianNodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9a79e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.echo_state_network import ESNRegressor, ESNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0467c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, PredefinedWeightsInputToNode, NodeToNode\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7258097",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rd = get_range_doppler(DEFAULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "582fd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 500\n",
    "layer2_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67c80351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilayered; i2n -> n2o, second layer has fewer (125) nodes\n",
    "\n",
    "i2n = InputToNode(hidden_layer_size=layer1_size, input_activation=\"tanh\")\n",
    "\n",
    "# layer1 = i2n.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6171882",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = {'hidden_layer_size': layer2_size, #1000 in 5C\n",
    "                  'input_activation': 'tanh', #\n",
    "                  'k_in' : 1,\n",
    "                  'bias_scaling': 0.0, # usually 0 \n",
    "                  'spectral_radius' : 0.0, # \n",
    "                  'reservoir_activation': 'tanh', # 2\n",
    "                  'leakage': 0.05, #equation 6, 17 frames should be 1 step \n",
    "                  'bidirectional': True, #bidirectional \n",
    "                  'k_rec': layer2_size / 10, # optimal is 200 for 1000 layer size \n",
    "                  'alpha': 0.1, # figure 6, tuned with k_in\n",
    "#                   'random_state': 1, # \n",
    "                  'requires_sequence': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60a03477",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Pipeline([\n",
    "    ('i2n', i2n), \n",
    "              ])\n",
    "n2n = Pipeline([('layer1', NodeToNode(hidden_layer_size=layer2_size, leakage=.05)),\n",
    "               ('layer2', NodeToNode(hidden_layer_size=layer2_size, leakage=.05))])\n",
    "n2o = ESNClassifier(**initial_params, input_to_node=l1, node_to_node=n2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69d1ed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESNClassifier(input_to_node=Pipeline(steps=[('i2n', InputToNode())]),\n",
       "              node_to_node=Pipeline(steps=[('layer1', NodeToNode(leakage=0.05)),\n",
       "                                           ('layer2',\n",
       "                                            NodeToNode(leakage=0.05))]),\n",
       "              regressor=IncrementalRegression(alpha=0.1),\n",
       "              requires_sequence=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2o.fit(dataset, labels_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97bdb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = n2o.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4dcc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for i, prediction in enumerate(predictions):  \n",
    "    \n",
    "    correct.append((prediction == labels[i]).mean())\n",
    "\n",
    "correct = np.array(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3dc80420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6478194175820455"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b4941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_p = n2o.predict_proba(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d321da2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130,), (179, 5))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_p.shape, predictions_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "831b5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,0,0,0,0]\n",
    "\n",
    "for p in predictions: \n",
    "    for i in range(5): \n",
    "        x[i] += (p == i+1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6db22ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15908, 21951, 16946, 18818, 22027]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371d545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0336b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f73796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6eccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48745a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a751971",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode, PredefinedWeightsInputToNode, NodeToNode\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, f1_score\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5d613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': layer2_size, #1000 in 5C\n",
    "                  'input_activation': 'tanh', #\n",
    "                  'k_in' : 1,\n",
    "                  # 'bias_scaling': 0.0, # usually 0 \n",
    "                  # 'spectral_radius' : 0.0, # \n",
    "                  'reservoir_activation': 'tanh', # 2\n",
    "                  # 'leakage': 0.05, #equation 6, 17 frames should be 1 step \n",
    "                  'bidirectional': True, #bidirectional \n",
    "                  # 'k_rec': layer2_size / 10, # optimal is 200 for 1000 layer size \n",
    "                  # 'alpha': 0.1, # figure 6, tuned with k_in\n",
    "                  'random_state': 1, # \n",
    "                  'requires_sequence': True}\n",
    "\n",
    "step1_esn_params = {'k_rec': uniform(10, layer2_size/2),\n",
    "                    'spectral_radius': uniform(loc=0, scale=1e-3)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': uniform(1e-5, 1e-1)}\n",
    "\n",
    "scorer = make_scorer(accuracy_score, greater_is_better=True, needs_proba=False)\n",
    "step_kwargs = {'n_iter': 50, 'random_state': 1, 'verbose': 1, 'n_jobs': -1, 'cv': TimeSeriesSplit(), 'scoring': scorer}\n",
    "\n",
    "kwargs_step1 = {**step_kwargs}\n",
    "kwargs_step2 = {**step_kwargs}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': scorer}\n",
    "kwargs_step4 = {**step_kwargs}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNClassifier(**initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"joblib/mse_sequential_search.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(dataset, labels_repeat)\n",
    "    dump(sequential_search, \"joblib/mse_sequential_search.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c543266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "esn = sequential_search.best_estimator_\n",
    "\n",
    "y_pred_train = esn.predict_proba(X=dataset)\n",
    "cm = confusion_matrix(np.argmax(np.concatenate(labels_hot_repeat), axis=1), np.argmax(np.concatenate(y_pred_train), axis=1))\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=[0, 1, 2, 3, 4]).plot()\n",
    "print(\"Classification training report for estimator %s:\\n%s\\n\" % (esn, classification_report(np.argmax(np.concatenate(labels_hot_repeat), axis=1), np.argmax(np.concatenate(y_pred_train), axis=1), digits=10)))\n",
    "\n",
    "# y_pred_test = esn.predict(X=X_test)\n",
    "# cm = confusion_matrix(np.argmax(np.concatenate(y_test), axis=1), np.argmax(np.concatenate(y_pred_test), axis=1))\n",
    "# cm_display = ConfusionMatrixDisplay(cm, display_labels=[0, 1, 2]).plot()\n",
    "# print(\"Classification training report for estimator %s:\\n%s\\n\" % (esn, classification_report(np.argmax(np.concatenate(y_test), axis=1), np.argmax(np.concatenate(y_pred_test), axis=1), digits=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a817ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from door example\n",
    "\n",
    "\n",
    "\n",
    "initially_fixed_params = {'hidden_layer_size': 100,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage': 0.1,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bidirectional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_km_esn = ESNClassifier(input_to_node=PredefinedWeightsInputToNode(predefined_input_weights=w_in.T),\n",
    "                                    **initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"RICSyN2015/sequential_search_RICSyN2015_km_large_1.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_km_esn, searches=searches).fit(dataset, labels_repeat)\n",
    "    dump(sequential_search, \"RICSyN2015/sequential_search_RICSyN2015_km_large_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_esn.fit(dataset, labels_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = base_esn.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa18d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for i, prediction in enumerate(predictions):  \n",
    "    \n",
    "    correct.append((prediction == labels[i]).mean())\n",
    "\n",
    "correct = np.array(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeedafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
